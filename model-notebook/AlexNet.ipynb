{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOiwWAliW7D4"
      },
      "source": [
        "# Comparison of Air Quality Index (AQI) Prediction Based on AlexNet, VGGNet, ResNet\n",
        "\n",
        "Kelompok 01 Kecerdasan buatan 02:\n",
        "* Fateen Najib Indramustika - 2006468522\n",
        "* Joshevan - 2006577321\n",
        "* Airell Ramadhan Budiraharjo - 2006535230"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnz1P7bkVNcl"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_C3E1McVrQJ"
      },
      "outputs": [],
      "source": [
        "def match_images_with_csv(csv_file_path, images_folder_path, timezone_offset=7):\n",
        "    timestamp_AQI_mapping = {}\n",
        "    with open(csv_file_path, mode='r') as csvfile:\n",
        "        csv_reader = csv.DictReader(csvfile)\n",
        "        for row in csv_reader:\n",
        "            now_timestamp = datetime.fromisoformat(row['Now Timestamp'])\n",
        "            now_timestamp += timedelta(hours=timezone_offset)\n",
        "            formatted_timestamp = now_timestamp.strftime('%Y%m%d_%H%M')\n",
        "            formatted_timestamp += \"00\"\n",
        "            timestamp_AQI_mapping[formatted_timestamp] = row['AQI']\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for image_file in os.listdir(images_folder_path):\n",
        "        if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(images_folder_path, image_file)\n",
        "            image_timestamp = image_file.split('.')[0]\n",
        "\n",
        "            # Match the image timestamp with the CSV data\n",
        "            if image_timestamp in timestamp_AQI_mapping:\n",
        "                AQI_value = timestamp_AQI_mapping[image_timestamp]\n",
        "                y.append(int(AQI_value))\n",
        "                x.append(image_path.split('\\\\')[-1])\n",
        "    data = {}\n",
        "    data['image_path'] = x\n",
        "    data['AQI'] = y\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSpvvywkWgVn",
        "outputId": "5318e080-6423-4ce5-fe32-a4fa74cae050"
      },
      "outputs": [],
      "source": [
        "data = match_images_with_csv('air_quality_data.csv', 'image')\n",
        "df_aqi = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing data for tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5s7Kn5aYbJg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(df_aqi, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGeEWcP6YsHp"
      },
      "outputs": [],
      "source": [
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_data,\n",
        "    directory='image/',\n",
        "    x_col='image_path',\n",
        "    y_col='AQI',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='raw'\n",
        ")\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    test_data,\n",
        "    directory='image/',\n",
        "    x_col='image_path',\n",
        "    y_col='AQI',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='raw',\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AlexNet for multiclass classification with 200 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4Dlo2EfituA",
        "outputId": "b64806d9-1158-4c99-98bd-005af0a1d959"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten,\n",
        "                                     Dense, Dropout, BatchNormalization)\n",
        "\n",
        "def alexnet_model(input_shape=(227, 227, 3)):\n",
        "    model = Sequential([\n",
        "        # 1st Convolutional Layer\n",
        "        Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "\n",
        "        # 2nd Convolutional Layer\n",
        "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "\n",
        "        # 3rd, 4th, and 5th Convolutional Layers\n",
        "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "\n",
        "        # Flattening Layer\n",
        "        Flatten(),\n",
        "\n",
        "        # 1st Dense Layer\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        # 2nd Dense Layer\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        # 3rd Dense Layer\n",
        "        Dense(1000, activation='relu'), \n",
        "        Dropout(0.5),\n",
        "\n",
        "        # Output Layer\n",
        "        Dense(200, activation='softmax') \n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = alexnet_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_data) // 32,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=len(test_data) // 32,\n",
        "    epochs=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_accuracy = history.history['accuracy'].copy()\n",
        "val_accuracy = history.history['val_accuracy'].copy()\n",
        "\n",
        "for i in range(0, len(train_accuracy)):\n",
        "    train_accuracy[i] *= 100\n",
        "\n",
        "for i in range(0, len(val_accuracy)):\n",
        "    val_accuracy[i] *= 100\n",
        "\n",
        "plt.plot(train_accuracy)\n",
        "plt.plot(val_accuracy)\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.text(80, val_accuracy[99], f'Validation Score: {val_accuracy[99]:.2f}', ha='center', va='bottom')\n",
        "plt.savefig('accuracy.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AlexNet for Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten,\n",
        "                                     Dense, Dropout, BatchNormalization)\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "def alexnet_model(input_shape=(227, 227, 3)):\n",
        "    model = Sequential([\n",
        "        # 1st Convolutional Layer\n",
        "        Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "\n",
        "        # 2nd Convolutional Layer\n",
        "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "\n",
        "        # 3rd, 4th, and 5th Convolutional Layers\n",
        "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "\n",
        "        GlobalAveragePooling2D(),\n",
        "\n",
        "        # Dense Layer with 1024 units\n",
        "        Dense(1024, activation='relu'),\n",
        "\n",
        "        # Dense Layers with 4096 units \n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        \n",
        "        # Output Layer for continuous value prediction\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = alexnet_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error',tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_batch, y_batch = next(train_generator)\n",
        "print(f'x_batch shape: {x_batch.shape}, dtype: {x_batch.dtype}')\n",
        "print(f'y_batch shape: {y_batch.shape}, dtype: {y_batch.dtype}')\n",
        "\n",
        "# Inspecting a batch from test_generator\n",
        "x_val_batch, y_val_batch = next(test_generator)\n",
        "print(f'x_val_batch shape: {x_val_batch.shape}, dtype: {x_val_batch.dtype}')\n",
        "print(f'y_val_batch shape: {y_val_batch.shape}, dtype: {y_val_batch.dtype}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_data) // 32,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=len(test_data) // 32,\n",
        "    epochs=100  # Adjust the number of epochs based on your needs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "val_lost = history.history['val_loss']\n",
        "lost = history.history['loss']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(val_lost)\n",
        "plt.plot(lost)\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss') \n",
        "plt.legend(['Validation Loss', 'Loss'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_rmse = history.history['mean_absolute_error']\n",
        "val_rmse = history.history['val_mean_absolute_error']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_rmse)\n",
        "plt.plot(val_rmse)\n",
        "plt.title('RMSE over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend(['Train RMSE', 'Validation RMSE'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_mae, test_rmse  = model.evaluate(test_generator, steps=len(test_data) // 32)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test MAE: {test_mae}\")\n",
        "print(f\"Test RMSE: {test_rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_rmse = history.history['mean_absolute_error']\n",
        "val_rmse = history.history['val_mean_absolute_error']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_rmse)\n",
        "plt.plot(val_rmse)\n",
        "plt.title('RMSE over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.text(450,200, f'Validation RMSE: {val_rmse[499]:.4f}', ha='center', va='bottom')\n",
        "plt.text(450,200, f'Testing RMSE: {train_rmse[499]:.4f}', ha='center', va='top')\n",
        "plt.legend(['Train RMSE', 'Validation RMSE'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_result = df_result.sort_index()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df_result['Actual AQI'], label='Actual AQI')\n",
        "plt.plot(df_result['Predicted AQI'], label='Predicted AQI')\n",
        "plt.title('Actual AQI vs Predicted AQI')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('AQI')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
